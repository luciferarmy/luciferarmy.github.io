<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[数据可视化工具]]></title>
    <url>%2F2018%2F05%2F25%2F%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96%E5%B7%A5%E5%85%B7%E7%AE%80%E4%BB%8B%2F</url>
    <content type="text"><![CDATA[&#160; &#160; &#160; &#160;大数据时代,拥有数据就拥有一切,python作为强大的数据分析利器,在数据分析领域独领风骚,下面介绍一个很好的数据可视化工具. ECharts&#160; &#160; &#160; &#160;Echarts是一个使用 JavaScript实现的开源可视化库，可以流畅的运行在 PC 和移动设备上，兼容当前绝大部分浏览器（IE8/9/10/11，Chrome，Firefox，Safari等），提供直观，交互丰富，可高度个性化定制的数据可视化图表。提供了常规的折线图、柱状图、散点图、饼图、K线图，用于统计的盒形图，用于地理数据可视化的地图、热力图、线图，用于关系数据可视化的关系图、treemap。 特点: 1.基于HTML5的图形库,图形的创建也比较简单，直接引用JS即可 2.百度的项目，而且一直有更新，目前最新的是EChart 4.1.0； 3.项目文档比较详细，而且是中文的，理解比较容易； 4.支持的图形很丰富，并且可以直接切换图形，使用起来非常方便 使用:&#160; &#160; &#160; &#160;官网下载echarts.js引入即可使用. ECharts首页:&#160; &#160; &#160; &#160;http://echarts.baidu.com pyecharts&#160; &#160; &#160; &#160;pyecharts 是一个用于生成 Echarts 图表的Python类库,可以实现在 Python 中直接使用数据生成图表 特点: 1.操作简单使用方便 2.语言翻译扩展pyecharts-javascripthon可以实现将python3.5+代码转换为JS代码 3.支持在Jupyter Notebook,Flask,Django平台中使用 (目前最新版本0.5.8) 使用:&#160; &#160; &#160; &#160;pip install pyecharts pyecharts主页:&#160; &#160; &#160; &#160;http://pyecharts.org 可以展示的图表类型:Bar（柱状图/条形图） Bar3D（3D 柱状图） Boxplot（箱形图） EffectScatter（带有涟漪特效动画的散点图） Funnel（漏斗图） Gauge（仪表盘） Geo（地理坐标系） GeoLines（地理坐标系线图） Graph（关系图） HeatMap（热力图） Kline/Candlestick（K线图） Line（折线/面积图） Line3D（3D 折线图） Liquid（水球图） Map（地图） Parallel（平行坐标系） Pie（饼图） Polar（极坐标系） Radar（雷达图） Sankey（桑基图） Scatter（散点图） Scatter3D（3D 散点图） ThemeRiver（主题河流图） Tree（树图） TreeMap（矩形树图） WordCloud（词云图） 1. 折线图123from pyecharts import onlineonline() 1234567891011121314151617181920from pyecharts import Lineattr = ['周一', '周二', '周三', '周四', '周五', '周六', '周日']line = Line("折线图示例")line.add( "最高气温", attr, [11, 11, 15, 13, 12, 13, 10], mark_point=["max", "min"], mark_line=["average"],)line.add( "最低气温", attr, [1, -2, 2, 5, 3, 2, 0], mark_point=["max", "min"], mark_line=["average"], yaxis_formatter="°C",)line 折线图 2. 柱状图123456789from pyecharts import Barattr = ["衬衫", "羊毛衫", "雪纺衫", "裤子", "高跟鞋", "袜子"]v1 = [5, 20, 36, 10, 75, 90]v2 = [10, 25, 8, 60, 20, 80]bar = Bar("柱状图数据堆叠")bar.add("商家A", attr, v1, is_stack=True)bar.add("商家B", attr, v2, is_stack=True)bar 柱状图数据堆叠 123456from pyecharts import Barbar = Bar("x 轴和 y 轴交换")bar.add("商家A", attr, v1)bar.add("商家B", attr, v2, is_convert=True)bar x 轴和 y 轴交换柱状图 3. Bar3D (3D柱状图)123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354from pyecharts import Bar3Dbar3d = Bar3D("3D 柱状图", width=900, height=600)x_axis = [ "12a", "1a", "2a", "3a", "4a", "5a", "6a", "7a", "8a", "9a", "10a", "11a", "12p", "1p", "2p", "3p", "4p", "5p", "6p", "7p", "8p", "9p", "10p", "11p" ]y_axis = [ "Saturday", "Friday", "Thursday", "Wednesday", "Tuesday", "Monday", "Sunday" ]data = [ [0, 0, 5], [0, 1, 1], [0, 2, 0], [0, 3, 0], [0, 4, 0], [0, 5, 0], [0, 6, 0], [0, 7, 0], [0, 8, 0], [0, 9, 0], [0, 10, 0], [0, 11, 2], [0, 12, 4], [0, 13, 1], [0, 14, 1], [0, 15, 3], [0, 16, 4], [0, 17, 6], [0, 18, 4], [0, 19, 4], [0, 20, 3], [0, 21, 3], [0, 22, 2], [0, 23, 5], [1, 0, 7], [1, 1, 0], [1, 2, 0], [1, 3, 0], [1, 4, 0], [1, 5, 0], [1, 6, 0], [1, 7, 0], [1, 8, 0], [1, 9, 0], [1, 10, 5], [1, 11, 2], [1, 12, 2], [1, 13, 6], [1, 14, 9], [1, 15, 11], [1, 16, 6], [1, 17, 7], [1, 18, 8], [1, 19, 12], [1, 20, 5], [1, 21, 5], [1, 22, 7], [1, 23, 2], [2, 0, 1], [2, 1, 1], [2, 2, 0], [2, 3, 0], [2, 4, 0], [2, 5, 0], [2, 6, 0], [2, 7, 0], [2, 8, 0], [2, 9, 0], [2, 10, 3], [2, 11, 2], [2, 12, 1], [2, 13, 9], [2, 14, 8], [2, 15, 10], [2, 16, 6], [2, 17, 5], [2, 18, 5], [2, 19, 5], [2, 20, 7], [2, 21, 4], [2, 22, 2], [2, 23, 4], [3, 0, 7], [3, 1, 3], [3, 2, 0], [3, 3, 0], [3, 4, 0], [3, 5, 0], [3, 6, 0], [3, 7, 0], [3, 8, 1], [3, 9, 0], [3, 10, 5], [3, 11, 4], [3, 12, 7], [3, 13, 14], [3, 14, 13], [3, 15, 12], [3, 16, 9], [3, 17, 5], [3, 18, 5], [3, 19, 10], [3, 20, 6], [3, 21, 4], [3, 22, 4], [3, 23, 1], [4, 0, 1], [4, 1, 3], [4, 2, 0], [4, 3, 0], [4, 4, 0], [4, 5, 1], [4, 6, 0], [4, 7, 0], [4, 8, 0], [4, 9, 2], [4, 10, 4], [4, 11, 4], [4, 12, 2], [4, 13, 4], [4, 14, 4], [4, 15, 14], [4, 16, 12], [4, 17, 1], [4, 18, 8], [4, 19, 5], [4, 20, 3], [4, 21, 7], [4, 22, 3], [4, 23, 0], [5, 0, 2], [5, 1, 1], [5, 2, 0], [5, 3, 3], [5, 4, 0], [5, 5, 0], [5, 6, 0], [5, 7, 0], [5, 8, 2], [5, 9, 0], [5, 10, 4], [5, 11, 1], [5, 12, 5], [5, 13, 10], [5, 14, 5], [5, 15, 7], [5, 16, 11], [5, 17, 6], [5, 18, 0], [5, 19, 5], [5, 20, 3], [5, 21, 4], [5, 22, 2], [5, 23, 0], [6, 0, 1], [6, 1, 0], [6, 2, 0], [6, 3, 0], [6, 4, 0], [6, 5, 0], [6, 6, 0], [6, 7, 0], [6, 8, 0], [6, 9, 0], [6, 10, 1], [6, 11, 0], [6, 12, 2], [6, 13, 1], [6, 14, 3], [6, 15, 4], [6, 16, 0], [6, 17, 0], [6, 18, 0], [6, 19, 0], [6, 20, 1], [6, 21, 2], [6, 22, 2], [6, 23, 6] ]range_color = ['#313695', '#4575b4', '#74add1', '#abd9e9', '#e0f3f8', '#ffffbf', '#fee090', '#fdae61', '#f46d43', '#d73027', '#a50026']bar3d.add( "", x_axis, y_axis, [[d[1], d[0], d[2]] for d in data], is_visualmap=True, visual_range=[0, 20], visual_range_color=range_color, grid3d_width=200, grid3d_depth=80,)bar3d 4.饼状图123456from pyecharts import Pieattr = ["衬衫", "羊毛衫", "雪纺衫", "裤子", "高跟鞋", "袜子"]v1 = [11, 12, 13, 10, 10, 10]pie = Pie("饼图示例")pie.add("", attr, v1, is_label_show=True)pie 饼图 123456789101112131415161718192021222324from pyecharts import Piepie = Pie('各类电影中"好片"所占的比例', "数据来着豆瓣", title_pos='center')pie.add("", ["剧情", ""], [25, 75], center=[10, 30], radius=[18, 24], label_pos='center', is_label_show=True, label_text_color=None, )pie.add("", ["奇幻", ""], [24, 76], center=[30, 30], radius=[18, 24], label_pos='center', is_label_show=True, label_text_color=None, legend_pos='left')pie.add("", ["爱情", ""], [14, 86], center=[50, 30], radius=[18, 24], label_pos='center', is_label_show=True, label_text_color=None)pie.add("", ["惊悚", ""], [11, 89], center=[70, 30], radius=[18, 24], label_pos='center', is_label_show=True, label_text_color=None)pie.add("", ["冒险", ""], [27, 73], center=[90, 30], radius=[18, 24], label_pos='center', is_label_show=True, label_text_color=None)pie.add("", ["动作", ""], [15, 85], center=[10, 70], radius=[18, 24], label_pos='center', is_label_show=True, label_text_color=None)pie.add("", ["喜剧", ""], [54, 46], center=[30, 70], radius=[18, 24], label_pos='center', is_label_show=True, label_text_color=None)pie.add("", ["科幻", ""], [26, 74], center=[50, 70], radius=[18, 24], label_pos='center', is_label_show=True, label_text_color=None)pie.add("", ["悬疑", ""], [25, 75], center=[70, 70], radius=[18, 24], label_pos='center', is_label_show=True, label_text_color=None)pie.add("", ["犯罪", ""], [28, 72], center=[90, 70], radius=[18, 24], label_pos='center', is_label_show=True, label_text_color=None, is_legend_show=True, legend_top="center")# pie.show_config()pie 组合饼图 5. 散点图123456789101112131415from pyecharts import Scatterv1 = [10, 20, 30, 40, 50, 60]v2 = [10, 20, 30, 40, 50, 60]scatter = Scatter("散点图")scatter.add("A", v1, v2)scatter.add( "B", v1[::-1], v2, is_visualmap=True, visual_type="size", visual_range_size=[20, 80],)scatter 散点图 6. gauge（仪表盘）12345from pyecharts import Gaugegauge = Gauge("仪表盘示例")gauge.add("业务指标", "完成率", 66.66)gauge 仪表盘图 7. 关系图12345678910111213141516171819from pyecharts import Graphimport osimport jsonwith open(os.path.join("fixtures", "weibo.json"), "r", encoding="utf-8") as f: j = json.load(f) nodes, links, categories, cont, mid, userl = jgraph = Graph("微博转发关系图", width=900, height=600)graph.add( "", nodes, links, categories, label_pos="right", graph_repulsion=50, is_legend_show=False, line_curve=0.2, label_text_color=None,)graph 微博转发关系图 8. 热力图123456789101112131415161718192021222324252627import datetimeimport randomfrom pyecharts import HeatMapbegin = datetime.date(2017, 1, 1)end = datetime.date(2017, 12, 31)data = [ [str(begin + datetime.timedelta(days=i)), random.randint(1000, 25000)] for i in range((end - begin).days + 1)]heatmap = HeatMap("日历热力图示例", "某人 2017 年微信步数情况", width=900)heatmap.add( "", data, is_calendar_heatmap=True, visual_text_color="#000", visual_range_text=["", ""], visual_range=[1000, 25000], calendar_cell_size=["auto", 30], is_visualmap=True, calendar_date_range="2017", visual_orient="horizontal", visual_pos="center", visual_top="80%", is_piecewise=True,)heatmap 日历热力图 9. K 线图123456789101112131415161718192021from pyecharts import Klinev1 = [[2320.26, 2320.26, 2287.3, 2362.94], [2300, 2291.3, 2288.26, 2308.38], [2295.35, 2346.5, 2295.35, 2345.92], [2347.22, 2358.98, 2337.35, 2363.8], [2360.75, 2382.48, 2347.89, 2383.76], [2383.43, 2385.42, 2371.23, 2391.82], [2377.41, 2419.02, 2369.57, 2421.15], [2425.92, 2428.15, 2417.58, 2440.38], [2411, 2433.13, 2403.3, 2437.42], [2432.68, 2334.48, 2427.7, 2441.73], [2430.69, 2418.53, 2394.22, 2433.89], [2416.62, 2432.4, 2414.4, 2443.03], [2441.91, 2421.56, 2418.43, 2444.8], [2420.26, 2382.91, 2373.53, 2427.07], [2383.49, 2397.18, 2370.61, 2397.94], [2378.82, 2325.95, 2309.17, 2378.82], [2322.94, 2314.16, 2308.76, 2330.88], [2320.62, 2325.82, 2315.01, 2338.78], [2313.74, 2293.34, 2289.89, 2340.71], [2297.77, 2313.22, 2292.03, 2324.63], [2322.32, 2365.59, 2308.92, 2366.16], [2364.54, 2359.51, 2330.86, 2369.65], [2332.08, 2273.4, 2259.25, 2333.54], [2274.81, 2326.31, 2270.1, 2328.14], [2333.61, 2347.18, 2321.6, 2351.44], [2340.44, 2324.29, 2304.27, 2352.02], [2326.42, 2318.61, 2314.59, 2333.67], [2314.68, 2310.59, 2296.58, 2320.96], [2309.16, 2286.6, 2264.83, 2333.29], [2282.17, 2263.97, 2253.25, 2286.33], [2255.77, 2270.28, 2253.31, 2276.22]]kline = Kline("K 线图示例")kline.add("日K", ["2017/7/&#123;&#125;".format(i + 1) for i in range(31)], v1)kline K 线图 10. Map (地图)12345678910111213141516from pyecharts import Mapvalue = [155, 10, 66, 78, 33, 80, 190, 53, 49.6]attr = [ "福建", "山东", "北京", "上海", "甘肃", "新疆", "河南", "广西", "西藏" ]map = Map("Map 结合 VisualMap 示例", width=1000, height=600)map.add( "", attr, value, maptype="china", is_visualmap=True, visual_text_color="#000",)map 11.词云图12345678910111213from pyecharts import WordCloudname = [ 'Sam S Club', 'Macys', 'Amy Schumer', 'Jurassic World', 'Charter Communications', 'Chick Fil A', 'Planet Fitness', 'Pitch Perfect', 'Express', 'Home', 'Johnny Depp', 'Lena Dunham', 'Lewis Hamilton', 'KXAN', 'Mary Ellen Mark', 'Farrah Abraham', 'Rita Ora', 'Serena Williams', 'NCAA baseball tournament', 'Point Break']value = [ 10000, 6181, 4386, 4055, 2467, 2244, 1898, 1484, 1112, 965, 847, 582, 555, 550, 462, 366, 360, 282, 273, 265]wordcloud = WordCloud(width=900, height=620)wordcloud.add("", name, value, word_size_range=[20, 100])wordcloud 词云图 12. Geo（地理坐标系）1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556from pyecharts import Geodata = [ ("海门", 9),("鄂尔多斯", 12),("招远", 12),("舟山", 12),("齐齐哈尔", 14),("盐城", 15), ("赤峰", 16),("青岛", 18),("乳山", 18),("金昌", 19),("泉州", 21),("莱西", 21), ("日照", 21),("胶南", 22),("南通", 23),("拉萨", 24),("云浮", 24),("梅州", 25), ("文登", 25),("上海", 25),("攀枝花", 25),("威海", 25),("承德", 25),("厦门", 26), ("汕尾", 26),("潮州", 26),("丹东", 27),("太仓", 27),("曲靖", 27),("烟台", 28), ("福州", 29),("瓦房店", 30),("即墨", 30),("抚顺", 31),("玉溪", 31),("张家口", 31), ("阳泉", 31),("莱州", 32),("湖州", 32),("汕头", 32),("昆山", 33),("宁波", 33), ("湛江", 33),("揭阳", 34),("荣成", 34),("连云港", 35),("葫芦岛", 35),("常熟", 36), ("东莞", 36),("河源", 36),("淮安", 36),("泰州", 36),("南宁", 37),("营口", 37), ("惠州", 37),("江阴", 37),("蓬莱", 37),("韶关", 38),("嘉峪关", 38),("广州", 38), ("延安", 38),("太原", 39),("清远", 39),("中山", 39),("昆明", 39),("寿光", 40), ("盘锦", 40),("长治", 41),("深圳", 41),("珠海", 42),("宿迁", 43),("咸阳", 43), ("铜川", 44),("平度", 44),("佛山", 44),("海口", 44),("江门", 45),("章丘", 45), ("肇庆", 46),("大连", 47),("临汾", 47),("吴江", 47),("石嘴山", 49),("沈阳", 50), ("苏州", 50),("茂名", 50),("嘉兴", 51),("长春", 51),("胶州", 52),("银川", 52), ("张家港", 52),("三门峡", 53),("锦州", 54),("南昌", 54),("柳州", 54),("三亚", 54), ("自贡", 56),("吉林", 56),("阳江", 57),("泸州", 57),("西宁", 57),("宜宾", 58), ("呼和浩特", 58),("成都", 58),("大同", 58),("镇江", 59),("桂林", 59),("张家界", 59), ("宜兴", 59),("北海", 60),("西安", 61),("金坛", 62),("东营", 62),("牡丹江", 63), ("遵义", 63),("绍兴", 63),("扬州", 64),("常州", 64),("潍坊", 65),("重庆", 66), ("台州", 67),("南京", 67),("滨州", 70),("贵阳", 71),("无锡", 71),("本溪", 71), ("克拉玛依", 72),("渭南", 72),("马鞍山", 72),("宝鸡", 72),("焦作", 75),("句容", 75), ("北京", 79),("徐州", 79),("衡水", 80),("包头", 80),("绵阳", 80),("乌鲁木齐", 84), ("枣庄", 84),("杭州", 84),("淄博", 85),("鞍山", 86),("溧阳", 86),("库尔勒", 86), ("安阳", 90),("开封", 90),("济南", 92),("德阳", 93),("温州", 95),("九江", 96), ("邯郸", 98),("临安", 99),("兰州", 99),("沧州", 100),("临沂", 103),("南充", 104), ("天津", 105),("富阳", 106),("泰安", 112),("诸暨", 112),("郑州", 113),("哈尔滨", 114), ("聊城", 116),("芜湖", 117),("唐山", 119),("平顶山", 119),("邢台", 119),("德州", 120), ("济宁", 120),("荆州", 127),("宜昌", 130),("义乌", 132),("丽水", 133),("洛阳", 134), ("秦皇岛", 136),("株洲", 143),("石家庄", 147),("莱芜", 148),("常德", 152),("保定", 153), ("湘潭", 154),("金华", 157),("岳阳", 169),("长沙", 175),("衢州", 177),("廊坊", 193), ("菏泽", 194),("合肥", 229),("武汉", 273),("大庆", 279)]geo = Geo( "全国主要城市空气质量", "data from pm2.5", title_color="#fff", title_pos="center", width=1000, height=600, background_color="#404a59",)attr, value = geo.cast(data)geo.add( "", attr, value, visual_range=[0, 200], visual_text_color="#fff", symbol_size=15, is_visualmap=True,)geo 1234567891011121314151617181920geo = Geo( "全国主要城市空气质量", "data from pm2.5", title_color="#fff", title_pos="center", width=1000, height=600, background_color="#404a59",)attr, value = geo.cast(data)geo.add( "", attr, value, type="heatmap", is_visualmap=True, visual_range=[0, 300], visual_text_color="#fff",)geo]]></content>
      <categories>
        <category>Data Aanalysis</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Data Analysis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[拉勾网python爬虫岗位招聘信息爬取和分析]]></title>
    <url>%2F2017%2F12%2F12%2Flagou%2F</url>
    <content type="text"><![CDATA[运用requests, pandas和pyecharts模块爬取数据做一个简单的招聘信息数据分析 1.爬虫爬取拉勾网 python爬虫岗位的招聘信息(深圳地区+爬虫),并将爬取到的数据保存在csv文件中,代码如下: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150"""拉勾网Python职位数据爬取"""import jsonimport randomimport reimport uuidimport requestsimport timeimport pymysqlimport pymysql.cursorsclass LagouSpider(): def __init__(self,city,kw): self.baseUrl = 'https://www.lagou.com/jobs/positionAjax.json?' self.kw=kw self.querystring = &#123;'px': 'new', 'city': city, 'needAddtionalResult': 'false'&#125; self.cookie = "JSESSIONID=" + self.get_uuid() + "; user_trace_token=" + self.get_uuid() + "; LGUID=" + self.get_uuid() + "; index_location_city=%E6%B7%B1%E5%9C%B3; SEARCH_ID=" + self.get_uuid() + '; _gid=GA1.2.717841549.1514043316; _ga=GA1.2.952298646.1514043316; LGSID=' + self.get_uuid() + "; LGRID=" + self.get_uuid() + "; " self.headers = &#123; 'Host': 'www.lagou.com', 'cookie': self.cookie, 'origin': "https://www.lagou.com", 'x-anit-forge-code': "0", 'accept-encoding': "gzip, deflate, br", 'accept-language': "zh-CN,zh;q=0.8,en;q=0.6", 'user-agent': "Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.139 Mobile Safari/537.36", 'content-type': "application/x-www-form-urlencoded; charset=UTF-8", 'accept': "application/json, text/javascript, */*; q=0.01", 'referer': "https://www.lagou.com/jobs/list_pyhton?labelWords=&amp;fromSearch=true&amp;suginput=", 'x-requested-with': "XMLHttpRequest", 'connection': "keep-alive", 'x-anit-forge-token': "None", 'cache-control': "no-cache" &#125; def makeUpBodyList(self): """构造body_list""" body_list = [] for page_num in [i for i in range(1, 31)]: body = &#123; 'first': 'true', 'pn': str(page_num), 'kd': self.kw &#125; body_list.append(body) return body_list def getResp(self, body): """对一个url发送请求获取响应并返回响应内容""" response = requests.request("POST", self.baseUrl, data=body, headers=self.headers, params=self.querystring) result = response.text data = json.loads(result) return data def get_content_list(self, data): """解析html页面,xpath提取数据并返回""" jobs_list = [] result_list = data['content']['positionResult']['result'] for result in result_list: if re.search(r'爬虫', result['positionName']): item = &#123;&#125; item['createTime'] = self.None2str(result['createTime']) item['workYear'] = self.None2str(result['workYear']) item['education'] = self.None2str(result['education']) item['positionName'] = self.None2str(result['positionName']).replace(',', '|') item['salary'] = self.None2str(result['salary']) item['companyFullName'] = self.None2str(result['companyFullName']) item['companySize'] = self.None2str(result['companySize']) item['positionLables'] = '|'.join(result['positionLables']) item['district'] = self.None2str(result['district']) item['industryField'] = self.None2str(result['industryField']).replace(',', '|') item['firstType'] = self.None2str(result['firstType']) item['secondType'] = self.None2str(result['secondType']) item['positionAdvantage'] = self.None2str(result['positionAdvantage']).replace(',', '|') item['companyLabelList'] = '|'.join(result['companyLabelList']) else: continue jobs_list.append(item) return jobs_list def get_uuid(self): # UUID是128位的全局唯一标识符，通常由32字节的字符串表示，它可以保证时间和空间的唯一性。 # 它通过MAC地址、时间戳、命名空间、随机数、伪随机数来保证生成ID的唯一性。UUID主要有五个算法。 # uuid4()是基于随机数 return str(uuid.uuid4()) def None2str(self, a): if a is None: return '' else: return a def writeFile(self, jobs_list): # 写入文档 with open('lagou_python.csv', 'a', encoding="utf-8") as fw: for item in jobs_list: print(item) fw.write( item['createTime'] + ',' + item['workYear'] + ',' + item['education'] + ',' + item['positionName'] + ',' + item['salary'] + ',' + item['companyFullName'] + ',' + item['companySize'] + ',' + item['positionLables'] + ',' + item['district'] + ',' + item['industryField'] + ',' + item['firstType'] + ',' + item['secondType'] + ',' + item['positionAdvantage'] + ',' + item['companyLabelList'] + '\n') def PushToDB(self): db = pymysql.connect(host='127.0.0.1', port=3306, user='root', passwd='admin123', db='spider', charset='utf8', cursorclass=pymysql.cursors.DictCursor) db.autocommit(True) cursor = db.cursor() fr = open('lagou_python.txt', 'r', encoding="utf-8") count = 0 for line in fr: count += 1 if count == 1: continue line = line.strip().split(',') print(type(line), line) cursor.execute( "insert into lagou(createTime,workYear,education,positionName,salary,companyFullName,companySize,positionLables,district,industryField,firstType,secondType,positionAdvantage,companyLabelList) values(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)", [line[0], line[1], line[2], line[3], line[4], line[5], line[6], line[7], line[8], line[9], line[10], line[11], line[12], line[13]]) def run(self): jobs_list = [] body_list = self.makeUpBodyList() with open('lagou_python.csv', 'a', encoding="utf-8") as fw: # 写入一行表头说明每个字段的含义 fw.write( 'createTime,workYear,education,positionName,salary,companyFullName,companySize,positionLables,district,industryField,firstType,secondType,positionAdvantage,companyLabelList\n') for body in body_list: data = self.getResp(body) jobs_list = self.get_content_list(data) self.writeFile(jobs_list) print('==========================') time.sleep(random.randint(1, 5)) return len(jobs_list) / (30 * 15)if __name__ == "__main__": spider = LagouSpider('深圳','python爬虫') hitRate=spider.run() # PushToDB() 2.使用jupyter notebook处理爬取到的数据,并使用统计图进行数据分析可视化展示,代码如下: 12345# 查看数据详情import pandas as pddf=pd.read_csv("lagou_python2.csv",encoding="utf-8")df.head() 12345678# 学历分布饼图from pyecharts import Pieeducation=pd.value_counts(df['education'])attr = education.index.tolist()v1=education.tolist()pie = Pie("学历分布情况")pie.add("", attr, v1, is_label_show=True)pie 1234567891011121314# 薪资分布图from pyecharts import Bar, Gridsalary=pd.value_counts(df['salary'])attr = salary.index.tolist()v1=salary.tolist()# print(attr)# print(v1)grid = Grid()bar = Bar("薪资分布图")bar.add("", attr, v1, is_datazoom_show=True, xaxis_interval=0, xaxis_rotate=30)# 把 bar 加入到 grid 中，并适当调整 grid_bottom 参数，使 bar 图整体上移grid.add(bar, grid_bottom="25%")grid 12345678910# 地区分布情况from pyecharts import Piedistrict=pd.value_counts(df['district'])attr = district.index.tolist()v1=district.tolist()# print(attr)# print(v1)pie = Pie("地区分布情况")pie.add("", attr, v1, is_label_show=True)pie 12345678910# 岗位分布情况from pyecharts import PiesecondType=pd.value_counts(df['secondType'])attr = secondType.index.tolist()v1=secondType.tolist()# print(attr)# print(v1)pie = Pie("岗位分布情况",title_pos='center')pie.add("", attr, v1, is_label_show=True,legend_orient="vertical",legend_pos="left",radius=[40, 75],center=[50, 60],)pie]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>数据分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Django时间数据与保存到MySQL的时间相差8小时问题]]></title>
    <url>%2F2017%2F09%2F20%2FDjango_mysql_datetime%2F</url>
    <content type="text"><![CDATA[最近在使用Django框架开发时,发现保存到MySQL的时间总是与实际时间相差8小时,出现的问题有如下两种: 问题一: 在测试使用mysql保存session时,发现保存在mysql中的session expire_time 总是与系统时间相差 8 小时 问题二: 创建模型类时,时间属性比如create_time通过models.DateTimeField类型,数据保存到MySQL的时间与系统时间相差8 小时 之前创建项目的时候 在 setting 文件中已经进行了本地化配置: 12LANGUAGE_CODE = 'zh-Hans'TIME_ZONE = 'Asia/Shanghai' 但是实际保存时间还是有时区的差异 原因: datetime是不包含timezone信息的,如果不需要在程序中特别处理时区（timezone-aware），在Django项目的settings.py文件中，可以直接设置为“USE_TZ = False” 解决办法在settings文件中设置 1USE_TZ = False]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[WSGI,web服务器,web框架之间的关系]]></title>
    <url>%2F2017%2F09%2F16%2FWSGI_webserver_webframwork%2F</url>
    <content type="text"><![CDATA[&#160; &#160; &#160; &#160;之前在学习flask框架时对于http请求在flask框架与wsgi以及web服务器之间传递和处理的流程,以及他们之间的关系一直很困惑,最近查阅了一些资料,整理出了web服务器，web框架与WSGI这三者之间的关系. &#160; &#160; &#160; &#160;简答的说:客户端从发送一个 HTTP 请求到 Flask 处理请求，分别经过了 web服务器层，WSGI层，web框架层，这三个层次。不同的层次其作用也不同，下面简要介绍各层的作用。 如图: Web服务器层Web服务器是一类特殊的服务器，其作用是主要是接收 HTTP 请求并返回响应. 要运行web应用，必须有 web server;比如我们熟悉的apache、nginx，或者python中的gunicorn; 在flask框架中,werkzeug模块提供了web server,即是WSGIServer，下图的黄色Server部分; Web框架层&#160; &#160; &#160; &#160;Web框架的作用主要是方便我们开发 web应用程序，HTTP请求的动态数据就是由 web框架层来提供的。&#160; &#160; &#160; &#160;常见的 web框架有Flask，Django等，我们以 Flask 框架为例子，展示 web框架的作用： 123456789from flask import Flaskapp = Flask(__name__)@app.route(&apos;/hello&apos;)def hello_world(): return &apos;Hello World!&apos;if __name__ == &apos;__main__&apos;: app.run(host=&apos;0.0.0.0&apos;, port=8080) &#160; &#160; &#160; &#160;以上代码就创建了一个web应用程序对象 app。使用Web框架,并不需要关心如何接收 HTTP 请求和如何将响应结果返回给用户,只需要关心如何实现业务的逻辑即可。每个Python Web应用都是一个可调用（callable）的对象。在 flask中，这个对象就是 app = Flask(name) 创建出来的 app，就是上图中的绿色Application部分; WSGI层&#160; &#160; &#160; &#160;WSGI 全称为 Web Server Gateway Interface,它不是服务器，只是一种接口,只适用于 Python 语言，其定义了web服务器和 web应用之间的接口规范。只要 web服务器和 web应用都遵守WSGI协议，那么 web服务器和 web应用就可以进行交互. WSGI接口编写示例: 123def application(env, start_response): start_response(&apos;200 OK&apos;, [(&apos;Content-Type&apos;, &apos;text/html&apos;)]) return [b&quot;Hello World&quot;] 上述代码就是一个完整的 WSGI接口，application()函数就是符合WSGI标准的一个HTTP处理函数 参数说明： env是一个字典，包含了类似 HTTP_HOST，HOST_USER_AGENT，SERVER_PROTOCO 等环境变量。 start_response则是一个方法，该方法接受两个参数，分别是status，response_headers。 &#160; &#160; &#160; &#160;application方法的主要作用是设置 http 响应的状态码和 Content-Type 等头部信息，并返回响应的具体结果。&#160; &#160; &#160; &#160;当一个支持 WSGI的 web服务器接收到客户端的请求后，便会调用这个 application 方法，并传给它两个参数，start_response 和 application息。 总结: WSGI将Web服务分成两个部分:服务器和应用程序。 WGSI服务器只负责与网络相关的两件事： 接收浏览器的HTTP请求、向浏览器发送HTTP应答；而对HTTP请求的具体处理逻辑，则通过调用WSGI应用程序进行。 值得指出的是，WSGI 是一种协议，需要区分几个相近的名词：uwsgi一种通讯协议，uWSGI服务器正是使用了uwsgi协议实现网络通讯 uWSGI实现了 uwsgi 和 WSGI 两种协议的web服务器。注意 uWSGI 本质上也是一种 web服务器，处于上面描述的三层结构中的 web服务器层。]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>wsgi</tag>
        <tag>Web服务</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[同步异步与阻塞非阻塞的区别]]></title>
    <url>%2F2017%2F07%2F16%2Fasync_note%2F</url>
    <content type="text"><![CDATA[&#160; &#160; &#160; &#160;对于阻塞非阻塞与同步异步的关系与区别一直不是很理解,于是在网上查找了下相关资料,在此记录下,以便后面回顾学习。 “阻塞”与”非阻塞”与”同步”与“异步”不能简单的从字面理解，提供一个从分布式系统角度的回答。 同步与异步 &#160; &#160; &#160; &#160;同步和异步关注的是消息通信机制 (synchronous communication/ asynchronous communication)&#160; &#160; &#160; &#160;所谓同步，就是在发出一个调用时，在没有得到结果之前，该调用就不返回。但是一旦调用返回，就得到返回值了。换句话说，就是由调用者主动等待这个调用的结果。 &#160; &#160; &#160; &#160;而异步则是相反，调用在发出之后，这个调用就直接返回了，所以没有返回结果。换句话说，当一个异步过程调用发出后，调用者不会立刻得到结果。而是在调用发出后，被调用者通过状态、通知来通知调用者，或通过回调函数处理这个调用。 典型的异步编程模型比如Node.js 举个通俗的例子：&#160; &#160; &#160; &#160;你打电话问书店老板有没有《分布式系统》这本书，如果是同步通信机制，书店老板会说，你稍等，”我查一下”，然后开始查啊查，等查好了（可能是5秒，也可能是一天）告诉你结果（返回结果）。而异步通信机制，书店老板直接告诉你我查一下啊，查好了打电话给你，然后直接挂电话了（不返回结果）。然后查好了，他会主动打电话给你。在这里老板通过“回电”这种方式来回调。 阻塞与非阻塞 阻塞和非阻塞关注的是程序在等待调用结果（消息，返回值）时的状态. 阻塞调用是指调用结果返回之前，当前线程会被挂起。调用线程只有在得到结果之后才会返回。 非阻塞调用指在不能立刻得到结果之前，该调用不会阻塞当前线程。 还是上面的例子，&#160; &#160; &#160; &#160;你打电话问书店老板有没有《分布式系统》这本书，你如果是阻塞式调用，你会一直把自己“挂起”，直到得到这本书有没有的结果，如果是非阻塞式调用，你不管老板有没有告诉你，你自己先一边去玩了， 当然你也要偶尔过几分钟check一下老板有没有返回结果。在这里阻塞与非阻塞与是否同步异步无关。跟老板通过什么方式回答你结果无关。 ==总结:== 阻塞，非阻塞：进程/线程要访问的数据是否就绪，进程/线程是否需要等待；同步，异步：访问数据的方式，同步需要主动读写数据，在读写数据的过程中还是会阻塞；异步只需要I/O操作完成的通知，并不主动读写数据，由操作系统内核完成数据的读写。 在处理 IO 的时候，阻塞和非阻塞都是同步 IO。只有使用了特殊的 API 才是异步 IO。]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[git pull和本地文件冲突]]></title>
    <url>%2F2017%2F05%2F25%2Fgit%20pull%E4%BB%A3%E7%A0%81%E5%86%B2%E7%AA%81%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[&#160; &#160; &#160; &#160;在使用git pull代码时，经常会碰到有冲突的情况，提示如下信息：error: Your local changes to ‘c/environ.c’ would be overwritten by merge. Aborting.Please, commit your changes or stash them before you can merge. &#160; &#160; &#160; &#160;这个意思是说更新下来的内容和本地修改的内容有冲突，先提交你的改变或者先将本地修改暂时存储起来。 处理的方式非常简单，主要是使用git stash命令进行处理，分成以下几个步骤进行处理。 1、先将本地修改存储起来$ git stash这样本地的所有修改就都被暂时存储起来 。其中stash@{0}就是刚才保存的标记。 2、pull内容暂存了本地修改之后，就可以pull了。$ git pull 3、还原暂存的内容$ git stash pop stash@{0}系统提示如下类似的信息：Auto-merging c/environ.cCONFLICT (content): Merge conflict in c/environ.c意思就是系统自动合并修改的内容，但是其中有冲突，需要解决其中的冲突。 4、解决文件中冲突的的部分&#160; &#160; &#160; &#160;打开冲突的文件，其中Updated upstream 和=====之间的内容就是pull下来的内容，====和stashed changes之间的内容就是本地修改的内容。&#160; &#160; &#160; &#160;碰到这种情况，git也不知道哪行内容是需要的，所以要自行确定需要的内容。解决完成之后，就可以正常的提交了。]]></content>
      <categories>
        <category>疑难杂症</category>
      </categories>
      <tags>
        <tag>git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何将flask项目部署到阿里云服务器]]></title>
    <url>%2F2017%2F04%2F06%2Fpublic_flask%2F</url>
    <content type="text"><![CDATA[本文介绍如何在阿里云部署和发布项目 &#160; &#160; &#160; &#160;基于ubuntu 16.04系统，使用 Gunicorn + Nginx 进行布署，云服务器为阿里云 1.购买阿里云服务器 选择云服务器:阿里云服务器 https://www.aliyun.com 个人免费获取 https://free.aliyun.com/ 创建服务器选择ubuntu16.04 64位的操作系统 2.配置实例 进入控制台,查看实例创建情况 重置root密码 为实例添加安全组,给安全组配置规则 开放80和5000端口 3.登录Linux服务器安装项目运行环境 先更新 apt 相关源 1sudo apt-get update mysql安装 12apt-get install mysql-serverapt-get install libmysqlclient-dev redis安装 1sudo apt-get install redis-server 安装虚拟环境 12pip install virtualenvpip install virtualenvwrapper 使得安装的virtualenvwrapper生效，编辑~/.bashrc文件，内容如下: 123456export WORKON_HOME=$HOME/.virtualenvsexport PROJECT_HOME=$HOME/workspacesource /usr/local/bin/virtualenvwrapper.sh# 使编辑后的文件生效source ~/.bashrc 安装flask项目依赖包 requirements文件 &#160; &#160; &#160; &#160;flask项目中可以包含一个 requirements.txt 文件，用于记录所有依赖包及其精确的版本号，以便在新环境中进行部署操作。 在虚拟环境使用以下命令将当前虚拟环境中的依赖包以版本号生成至文件中： pip freeze &gt; requirements.txt &#160; &#160; &#160; &#160;当需要创建这个虚拟环境的完全副本，可以创建一个新的虚拟环境，并在其上运行以下命令： pip install -r requirements.txt &#160; &#160; &#160; &#160;在安装 Flask-MySQLdb 的时候可能会报错，可能是依赖包没有安装，执行以下命令安装依赖包： sudo apt-get build-dep python-mysqldb 4.Nginx安装和配置作用:实现分流、转发、负载均衡 安装 sudo apt-get install nginx 运行及停止 /etc/init.d/nginx start #启动 /etc/init.d/nginx stop #停止 配置 编辑文件: /etc/nginx/sites-available/default 配置并修改 location 节点下面的 proxy_pass 1234567891011121314151617181920212223#多机部署需要添加下面内容#upstream flask &#123;# server 127.0.0.1:5000;# server 127.0.0.1:5001;#&#125;server &#123; # 监听80端口 listen 80 default_server; listen [::]:80 default_server; root /var/www/html; index index.html index.htm index.nginx-debian.html; server_name _; location / &#123; # 请求转发到gunicorn服务器 proxy_pass http://127.0.0.1:5000; # 请求转发到多个gunicorn服务器(多机部署添加) # proxy_pass http://flask; &#125;&#125; 5.Gunicorn安装 &#160; &#160; &#160; &#160;Gunicorn（绿色独角兽）是一个Python WSGI的HTTP服务器,从Ruby的独角兽（Unicorn ）项目移植,该Gunicorn服务器与各种Web框架兼容，实现非常简单，轻量级的资源消耗 Gunicorn直接用命令启动，不需要编写配置文件 安装 pip install gunicorn 查看选项 gunicorn -h 6.运行项目gunicorn -w 2 -b 127.0.0.1:5000 运行文件名称:Flask程序实例名 -w: 表示进程（worker） -b：表示绑定ip地址和端口号（bind） OK,部署成功,快让其他人去访问一下你的网站吧~~]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>Lunix</tag>
        <tag>flask</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Centos7 安装mysql后登录提示：Access denied for user 'root'@'localhost']]></title>
    <url>%2F2017%2F04%2F01%2FCentos7%20%E5%AE%89%E8%A3%85mysql%E5%BC%82%E5%B8%B8%2F</url>
    <content type="text"><![CDATA[&#160; &#160; &#160; &#160;安装mysql后登录提示：ERROR 1045 (28000): Access denied for user ‘root‘@’localhost’ (using password:yes) 解决如下： 1.停止mysql服务 &#160; &#160; &#160; &#160;systemctl stop mysqld.service 2.修改配置文件无密码登录 &#160; &#160; &#160; &#160;vim /etc/my.cnf&#160; &#160; &#160; &#160;在最后加上skip-grant-tables,保存 3.启动mysql &#160; &#160; &#160; &#160;systemctl start mysqld.service 4.登录mysql &#160; &#160; &#160; &#160;mysql -u root &#160; &#160; &#160; &#160;注意这里不要加-p 5.修改密码，mysql5.7用此语法 &#160; &#160; &#160; &#160;use mysql; &#160; &#160; &#160; &#160;update mysql.user set authentication_string=password(‘123456’) where user=’root’ ; 6.回到第二部把刚加的那句删掉 &#160; &#160; &#160; &#160;保存，重启mysql就可以了]]></content>
      <categories>
        <category>疑难杂症</category>
      </categories>
      <tags>
        <tag>Linux</tag>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Http报头Accept与Content-Type的区别]]></title>
    <url>%2F2017%2F01%2F16%2Fhttp%2F</url>
    <content type="text"><![CDATA[​ 最近在使用Django DRF框架开发web后端HTTP接口时,发现一个以前忽略的小知识,今天有时间于是就写下来分享下. &#160; &#160; &#160; &#160;在Django DRF框架中, REST framework 提供了Parser解析器，在接收到请求后会自动根据Content-Type指明的请求数据类型（如JSON、表单等）将请求数据进行parse解析，解析为类字典对象保存到Request对象中。REST framework还提供了Renderer 渲染器，用来根据请求头中的Accept（接收数据类型声明）来自动转换响应数据到对应格式并保存到Response响应对象中。 那么,对于HTTP请求和响应报文,Content-Type是请求头中的属性,Accept是响应头中的属性吗? 查询了相关资料,得到如下结论: Accept属于请求头， Content-Type属于实体头。 Http报头分为通用报头，请求报头，响应报头和实体报头。请求方的http报头结构：通用报头|请求报头|实体报头响应方的http报头结构：通用报头|响应报头|实体报头 Accept代表发送端（客户端）希望接受的数据类型。 比如：Accept：text/xml;代表客户端希望接受的数据类型是xml类型 Content-Type代表发送端（客户端|服务器）发送的数据类型。 比如：Content-Type：text/html;代表发送端发送的数据格式是html。 二者合起来，Accept:text/xml；Content-Type:text/html即代表希望接受的数据类型是xml格式，本次请求发送的数据的数据格式是html。]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nginx静态资源和动态资源的分离]]></title>
    <url>%2F2016%2F09%2F16%2FNginx_static_atuo%2F</url>
    <content type="text"><![CDATA[&#160; &#160; &#160; &#160;Nginx是一款高性能的http 服务器,其特点是能够支持高并发(官方测试结果为5万并发)，并且cpu、内存等资源消耗却非常低，运行非常稳定. &#160; &#160; &#160; &#160;在web项目的中,我们经常使用nginx做分流和负载均衡.当网站的访问量达到一定程度后,为了解决并发，就可以使用nginx做负载均衡,提高网站并发量,解决性能瓶颈.实际上,nginx可以实现静态资源和动态资源的分离,将静态资源交给Nginx处理，而把动态资源转发给具体的web服务器进行处理,从而提高服务器处理效率.那么,该如何实现呢? 其实就是对nginx的location进行配置了.如下整理了关于配置的一些资料,分享下并作为笔记记录供以后学习和参考. 语法规则： 123456789101112131415location [=|~|~*|^~] /uri/ &#123; … &#125;= 开头表示精确匹配^~ 开头表示uri以某个常规字符串开头，理解为匹配 url路径即可。nginx不对url做编码，因此请求为/static/20%/aa，可以被规则^~ /static/ /aa匹配到（注意是空格）。~ 开头表示区分大小写的正则匹配~* 开头表示不区分大小写的正则匹配!~和!~*分别为区分大小写不匹配及不区分大小写不匹配 的正则/ 通用匹配，任何请求都会匹配到 多个location配置的情况下匹配顺序为： 首先匹配 =，其次匹配^~, 其次是按文件中顺序的正则匹配，最后是交给 / 通用匹配。当有匹配成功时候，停止匹配，按当前匹配规则处理请求。 举例: 123456789101112131415161718192021222324location = / &#123; #规则A&#125;location = /login &#123; #规则B&#125;location ^~ /static/ &#123; #规则C&#125;location ~ \.(gif|jpg|png|js|css)$ &#123; #规则D&#125;location ~* \.png$ &#123; #规则E&#125;location !~ \.xhtml$ &#123; #规则F&#125;location !~* \.xhtml$ &#123; #规则G&#125;location / &#123; #规则H&#125; 那么产生的效果如下： 123456789访问根目录/， 比如 http://localhost/ 将匹配规则A访问 http://localhost/login 将匹配规则B，http://localhost/register 则匹配规则H访问 http://localhost/static/a.html 将匹配规则C访问 http://localhost/a.gif, http://localhost/b.jpg 将匹配规则D和规则E，但是规则D顺序优先，规则E不起作用访问 http://localhost/static/c.png 则优先匹配到规则C访问 http://localhost/a.PNG 则匹配规则E，而不会匹配规则D，因为规则E不区分大小写。访问 http://localhost/a.xhtml 不会匹配规则F和规则G访问 http://localhost/a.XHTML不会匹配规则G，因为不区分大小写。规则F，规则G属于排除法，符合匹配规则但是不会匹配到，所以想想看实际应用中哪里会用到。 实际使用中，个人觉得至少有三个匹配规则定义，如下: 123456789101112131415161718192021#直接匹配网站根，通过域名访问网站首页比较频繁，使用这个会加速处理，官网如是说。#这里是直接转发给后端应用服务器了，也可以是一个静态首页# 第一个必选规则location = / &#123; proxy_pass http://tomcat:8080/index&#125;# 第二个必选规则是处理静态文件请求，这是nginx作为http服务器的强项# 有两种配置模式，目录匹配或后缀匹配,任选其一或搭配使用location ^~ /static/ &#123; # 静态资源都在 static 文件夹下 root /webroot/static/;&#125;location ~* \.(gif|jpg|jpeg|png|css|js|ico)$ &#123; root /webroot/res/;&#125;#第三个规则就是通用规则，用来转发动态请求到后端应用服务器#非静态文件请求就默认是动态请求，自己根据实际把握#毕竟目前的一些框架的流行，带.php,.jsp后缀的情况很少了location / &#123; proxy_pass http://tomcat:8080/&#125;]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo+GitHub 搭建个人博客]]></title>
    <url>%2F2016%2F09%2F07%2Fhexo_blog%2F</url>
    <content type="text"><![CDATA[1.Hexo安装Hexo官网有对Hexo安装及使用的详细介绍,安装Hexo前需要安装Node.js和Git1.Node.js 用来生成静态页面。Node.js官网，下载v6.10.0 LTS 一路安装即可。 2.Git 用来将本地Hexo内容提交到Github上。 3.安装Hexo 当Node.js和Git都安装好后就可以正式安装Hexo了，终端执行如下命令： npm install -g hexo-cli 初始化从cmd终端cd到一个你选定的目录，执行hexo init命令：hexo init myblog myblog是你建立的文件夹名称。cd到myblog文件夹下，执行如下命令，安装npm：npm install 执行如下命令，开启hexo服务器： hexo s 此时，浏览器中打开网址 http://localhost:4000 能看到如下页面： 本地设置好后，接下来开始关联Github。 2.关联Github1.创建仓库 登录你的Github帐号，新建仓库，名为用户名.github.io固定写法，如luciferarmy.github.io即下图中所示： 2.配置myblog本地的myblog文件夹下内容为： _config.yml db.json node_modules package.json scaffolds source themes 终端cd到myblog文件夹下，vim打开_config.yml，命令如下： vim _config.yml 打开后往下滑到最后，修改成下边的样子：1234deploy: type: git repository: https://github.com/luciferarmy/luciferarmy.github.io.git branch: master &#160; &#160; &#160; &#160;你需要将repository后luciferarmy换成你自己的用户名，地址在GitHub仓库获取。hexo 3.1.1版本后type:值为git。 &#160; &#160; &#160; &#160;注意：在配置所有的_config.yml文件时（包括theme中的），在所有的冒号:后边都要加一个空格，否则执行hexo命令会报错，切记 切记 切记！ 在myblog文件夹目录下执行生成静态页面命令： hexo generate 或者：hexo g 此时若出现如下报错： ERROR Local hexo not found in ~/blog ERROR Try runing: &apos;npm install hexo --save&apos; 则执行命令： npm install hexo --save 若无报错，自行忽略此步骤。 再执行配置命令： hexo deploy 或者：hexo d 注意：若执行命令hexo deploy仍然报错：无法连接git或找不到git，则执行如下命令来安装hexo-deployer-git： npm install hexo-deployer-git --save 再次执行hexo generate和hexo deploy命令。若你未关联Github，则执行hexo deploy命令时终端会提示你输入Github的用户名和密码，即 Username for &apos;https://github.com&apos;: GitHub用户名 Password for &apos;https://github.com&apos;: GitHub密码 hexo deploy 命令执行成功后，浏览器中打开网址 http://luciferarmy.github.io （将 luciferarmy 换成你的用户名）能看到和打开 http://localhost:4000 时一样的页面。 3.发布文章终端cd到myblog文件夹下，执行如下命令新建文章： hexo new &quot;firstArticle&quot; 名为firstArticle.md的文件会建在目录/blog/source/_ posts下，firstArticle是文件名，为方便链接不建议掺杂汉字。你当然可以用vim来编辑文章。还可以用Mou、Atom编辑器，支持预览！ 文章编辑完成后，终端cd到myblog文件夹下，执行如下命令来发布： hexo generate //生成静态页面 hexo deploy //将文章部署到Github 至此，Mac上搭建基于Github的Hexo博客就完成了。下面的内容是介绍安装theme和绑定个人域名，如果有兴趣且还有耐心的话，请继续吧。 4.安装Theme&#160; &#160; &#160; &#160;你可以到Hexo官网主题页去搜寻自己喜欢的theme。这里以hexo-theme-next为例 终端cd到 myblog 目录下执行如下命令： git clone https://github.com/iissnan/hexo-theme-next themes/next 将myblog目录下_config.yml里theme的名称landscape修改为next 终端cd到myblog目录下执行如下命令(每次部署文章的步骤)： 1234hexo clean //清除缓存文件 (db.json) 和已生成的静态文件 (public)hexo s --debug //debug调试模式(可在本地调试成功后发布)hexo g //生成缓存和静态文件hexo d //重新部署到服务器 至于更改theme内容比如名称、描述、头像等去修改myblog/_ config.yml 文件和myblog/themes/next/_ config.yml 文件中对应的属性名称即可，不要忘记冒号: 后加空格。NexT 使用文档里有极详细的介绍。 5.绑定个人域名&#160; &#160; &#160; &#160;现在使用的域名是Github提供的二级域名，也可以绑定为自己的个性域名。购买域名，可以到阿里万网，可直接在其网站做域名解析。 1.Github端&#160; &#160; &#160; &#160;在/myblog/themes/next/source目录下新建文件名为：CNAME文件，注意没有后缀名！直接将自己的域名如：luciferarmy.com写入。 2.域名解析&#160; &#160; &#160; &#160;如果将域名指向一个域名，实现与被指向域名相同的访问效果，需要增加CNAME记录。 登录阿里云，在你购买的域名后边点击：解析 –&gt; 添加解析 记录类型：CNAME 主机记录：将域名解析为example.com（不带www），填写@或者不填写 记录值：luciferarmy.github.io. (不要忘记最后的.，luciferarmy改为你自己的用户名)，点击保存即可，如下图： &#160; &#160; &#160; &#160;此时，点击访问http://luciferarmy.com和访问http://luciferarmy.github.io效果一致。 OK，大功告成！ 参考链接: Hexo-NexT配置超炫网页效果 hexo的next主题个性化配置教程 Hexo+NexT 主题配置备忘]]></content>
      <categories>
        <category>笔记</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[“钢铁牢笼”－《新教伦理与资本主义精神》有感]]></title>
    <url>%2F2016%2F08%2F24%2Fweber%2F</url>
    <content type="text"><![CDATA[&#160; &#160; &#160; &#160;信仰与理性之间是什么关系，可能会有很多人认为这两者之间是充满张力的对立的矛盾的两相。然而，韦伯在《新教伦理与资本主义精神》中就通过缜密的逻辑分析论证了宗教伦理与经济制度之间强烈的内在亲缘性。基督新教特有的宗教观念将“神恩蒙宠”视为一种身份，被选中的人将获得神的救赎而达到彼世，而此一身份的拥有必须证明自身的行为举止有异于“自然人”的生活方式。因此，在每位信徒身上一种以系统地讲求方法地审视自身的恩宠状态为动机的禁欲化的生活方式逐渐形成，最终导致以神的意志为导向的理性生活方式的形成。&#160; &#160; &#160; &#160;这种宗教观念渗透在信徒的个人生活和伦理判断中，逐渐形成了有系统的自我检视，塑造出一套首尾一贯笼罩整体的追求秩序和方法的生活方式，最终导致个人和社会整体的理性化。这种理性化的“入世苦行”的生活方式产生了一种新的财富观：认为获取财富并不是为了此世的享乐，而是最终指向一个信仰，即他们信仰的神，以世俗劳动作为禁欲苦行的最高手段，抵制自由享乐，对抗财产的非理性消费，以致富为终极目标而追求财富，荣耀上帝以求得到神的拣选而获得救赎。而这种禁欲的强制节制最终导致资本的形成，资本主义精神也由此产生。 &#160; &#160; &#160; &#160;这种建立在特殊宗教伦理基础之上的资本主义精神的确起到了积极的推动作用，直接导致了前所未有的生产方式和经济组织方式的形成，带来极大的财富和便利。然而在纯正的宗教热潮过了巅峰时期之时，追求天国理想的奋斗开始慢慢消解成冷静的职业道德，宗教根基逐渐枯萎，被功利的现世执着取代。这种“断根”的资本主义下，“朝圣人”被“经济人”取代，形成一种独特的市民职业风格。资本主义制度的双重性、内在矛盾与悖论性暴露无疑。正如韦伯所说，“现今的资本主义经济秩序是个巨大的宇宙，个人呱呱坠地于其中，对他而言，至少作为个体，这是个他必须生活在里头的既存的、事实上如铜墙铁壁般的桎梏。这宇宙强迫个人奉行其经济行为的规范，只要个人是卷入市场关系中的话。”资本主义的强制性以如钢铁般牢笼，每个置身其中的人都无力逃脱。 &#160; &#160; &#160; &#160;正如当今我们这个时代所看到的资本主义经济秩序，这种铜墙铁壁般的桎梏，不仅仅是物质上、生存上的枷锁，也包含在资本主义发展过程中逐渐形成的价值牢笼。这种价值观溯源于书中描述的“资本主义精神”，但是其内核随着资本主义经济秩序占据了社会的主导地位之后，其宗教基础逐渐淡化，取而代之的，是一种设计的、假想的社会价值观念。能够调动社会大量资源的资产运作者，将原有资本主义的观念进行改造重组，形成一种有利于资本发展的新观念体系，借助社会这个庞大的传播系统广而告之，不断的刷新人们的三观，于潜移默化之中改变着普罗大众的价值观，让个体不仅仅在物质上受到钳制，在价值观上还受到周遭社会环境的压迫，不得不投身于资本主义经济秩序，努力成为资本所青睐的增值工具。 &#160; &#160; &#160; &#160;这种价值观导致追求资本和财富实现极大利益获取的过程被社会赋予了极大的认同。人们被新的社会观念和众人的眼光压迫着，追求更好的成绩，更高的工资，更多的财富，从而为市场经济创造更多的价值，而自身的实际追求和价值观，则在纷繁迷人的成功学概念中被淹没掉了。说到这里，我再一次想起了一部电影《Fight Club》，这部电影所表现出的对现今资本主义精神的极度厌恶正是这深处牢笼之中被奴役的人类的反思。 &#160; &#160; &#160; &#160;Advertising has its taste in cars and clothes. Working jobs we hate so we can buy shit we don’t need. We’re the middle children of history. No purpose or place. We have no great war, no great depression. Our great war’s a spiritual war. Our great depression is our lives. We’ve all been raised on television to believe that one day we’d all be millionaires and movie gods and rock stars. But we won’t. We’re slowly learning that fact. And we’re very, very pissed off.(广告诱惑我们买车子，衣服，于是拼命工作买不需要的东西，我们是被历史遗忘的一代，没有目的，没有地位，没有世界大战，没有经济大恐慌，我们的大战只是心灵之战，我们的恐慌只是我们的生活。我们从小看电视，相信有一天会成为富翁，明星或摇滚巨星，但是，我们不会。那是我们逐渐面对着的现实，所以我们非常愤怒。) &#160; &#160; &#160; &#160;Why do guys like you and I know what a duvet is? Is this essential to our survival in the hunter-gatherer sense of the world? No. What are we, then? We’re consumers. We are by-products of a lifestyle obsession. Murder, crime, poverty…these things don’t concern me. What concerns me are celebrity magazines, television with 500 channels, some guy’s name on my underwear. Rogaine, viagra, olestra.(我们一定要知道鸭绒垫子吗？我们一定要字字斟酌吗？不。那你和我算什么？我们是消费者，我们满脑子想的都是物质。我不关心凶杀案和贫穷问题，我只关心名人杂志，500个频道的电视，我内裤上印着谁的名字。生发剂，威尔钢，减肥药。) &#160; &#160; &#160; &#160;”The things you own end up owning you.”,在表达对资本主义咆哮般的怒斥的同时,恰恰也表现出被钢铁牢笼无情禁锢却又无法逃脱其中的无奈.]]></content>
      <categories>
        <category>哲学宗教</category>
      </categories>
      <tags>
        <tag>读书</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2016%2F08%2F23%2Fhello-world%2F</url>
    <content type="text"><![CDATA[博客终于搭建起来了,写点小文章,欢迎大家来访~(_^^_)~]]></content>
      <categories>
        <category>日记</category>
      </categories>
      <tags>
        <tag>杂谈</tag>
      </tags>
  </entry>
</search>
